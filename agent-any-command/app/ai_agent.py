from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_community.callbacks.manager import get_openai_callback
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.types import Command
import os
import sys
import json
import asyncio
import traceback
from dotenv import load_dotenv
from add_interrupt import add_human_in_the_loop, human_in_the_loop_prompt

# OpenAI APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãŠã
# .envã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€OPENAI_API_KEY=<API KEY>ã‚’è¨˜è¼‰ã™ã‚‹
load_dotenv()

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
# max_tokensã§ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³é‡ã«åˆ¶é™ã‚’ã‹ã‘ã‚‹
# max_token*100ãã‚‰ã„ãŒå®Ÿéš›ã®åˆ¶é™å€¤ï¼Ÿ
llm = ChatOpenAI(
    model_name="gpt-5",
    temperature=0,
    max_tokens=100000,
)

initial_messages = [
    SystemMessage(
        content=(
            "ã‚ãªãŸã¯å„ªç§€ãªã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚"
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿æ›¸ãã—ãŸã„ã¨ãã¯/tmpãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚"
            "exec_shell_commandãƒ„ãƒ¼ãƒ«ã§ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã§ãã¾ã™ã€‚"
        )
    ),
]

client = MultiServerMCPClient(
    {
        "ExecuteCommand": {
            "command": "python",
            "args": ["/app/exec_mcp.py"],
            "transport": "stdio",
        },
    }
)


async def agent_invoke(agent, messages):
    response = None
    thread_config = {"configurable": {"thread_id": "1"}}

    with get_openai_callback() as cb:
        response = await agent.ainvoke(
            {"messages": messages},
            thread_config
        )

        while True:
            interrupt = response.get("__interrupt__")
            if interrupt is None:
                break
            else:
                cmd = human_in_the_loop_prompt(interrupt)
                response = await agent.ainvoke(cmd, thread_config)

        for msg in response["messages"]:
            msg.pretty_print()

        print(f"ğŸ”¢ ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {cb.total_tokens}")
        print(f"ğŸ“¥ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {cb.prompt_tokens}")
        print(f"ğŸ“¤ å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {cb.completion_tokens}")
        print(f"ğŸ’° æ¨å®šã‚³ã‚¹ãƒˆ: ${cb.total_cost:.6f}")

    return response


async def main():
    tools = await client.get_tools()

    # human in the loopã‚’toolsã«è¿½åŠ 
    checked_tools = []
    for tool in tools:
        checked_tool = add_human_in_the_loop(tool)
        checked_tools.append(checked_tool)

    # human in the loopç”¨ã®ãƒ¡ãƒ¢ãƒªæ©Ÿèƒ½
    checkpointer = InMemorySaver()

    agent = create_react_agent(
        model=llm,
        tools=checked_tools,
        checkpointer=checkpointer
    )

    messages = initial_messages
    print("ğŸ” å…¥åŠ›ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼ˆ'exit', 'quit', ç©ºå…¥åŠ›ã§çµ‚äº†ï¼‰")

    while True:
        old_messages = messages.copy()
        user_input = input("user>>> ").strip()
        if user_input in ("exit", "quit", ""):
            print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™")
            break
        try:
            human_message = HumanMessage(content=user_input)
            messages.append(human_message)

            # InMemorySaverã§çŠ¶æ…‹ä¿æŒã®ãŸã‚æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿è¿½åŠ 
            result_response = await agent_invoke(agent, [human_message])
            messages = result_response["messages"]
        except Exception as e:
            print(f"[âœ—] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            messages = old_messages

if __name__ == "__main__":
    asyncio.run(main())

